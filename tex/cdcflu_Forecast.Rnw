\documentclass[ijerph,article,submit,moreauthors,pdftex]{Definitions/mdpi}
%% For supplementary material, replace "article" by "supfile"
%% For arXiv, use:
%% \documentclass[preprints,article,accept,moreauthors,pdftex]{Definitions/mdpi}
%% \pdfoutput=1

%=================================================================
\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
%\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers
%\continuouspages{yes}

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz
\usepackage{bm}
\graphicspath{{../Results/plots/}}
\DeclareMathOperator{\DSS}{DSS}
\DeclareMathOperator{\LS}{LS}
\DeclareMathOperator{\AEE}{AE}
%=================================================================
% Full title of the paper (Capitalized)
\Title{Forecasting Flu Activity in the United States: Benchmarking an Endemic-Epidemic Beta Model}

% Author Orchid ID: enter ID or remove command
%\newcommand{\orcidauthorA}{0000-0000-000-000X}
\newcommand{\orcidauthorB}{0000-0002-1791-9449}

% Authors, for the paper (add full first names)
\Author{Junyi Lu and Sebastian Meyer *\orcidB{}}

% Authors, for metadata in PDF
\AuthorNames{Junyi Lu and Sebastian Meyer}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
Institute of Medical Informatics, Biometry, and Epidemiology,
Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg,
Erlangen, Germany}

% Contact information of the corresponding author
\corres{Correspondence: seb.meyer@fau.de}

% Abstract (max. 200 words, no \\)
\abstract{
  % Background:
  Accurate prediction of flu activity enables health officials to plan
  disease prevention and allocate treatment resources.
  % Methods:
  A promising forecasting approach is to adapt the well-established
  endemic-epidemic modelling framework to time series of infectious
  disease proportions.
  Using U.S. influenza-like illness surveillance data over 18 seasons,
  we assessed probabilistic forecasts of this new beta autoregressive
  model with proper scoring rules. Other readily available forecasting
  tools were used for comparison, including Prophet, (S)ARIMA, and kernel
  conditional density estimation (KCDE).
  % Results:
  Short-term flu activity up to four weeks ahead was equally well
  predicted by the beta model with four autoregressive lags and by KCDE,
  but the beta model runs much faster. Non-dynamic Prophet scored worst.
  Relative performance differed for seasonal peak prediction.
  Prophet produced the best peak intensity forecasts
  in seasons with standard epidemic curves, otherwise KCDE outperformed
  all other methods. Peak timing was best predicted by SARIMA, KCDE, or
  the Beta model, depending on the season. The best overall performance in
  predicting peak timing and intensity was achieved by KCDE.
  Only KCDE and naive historical forecasts %were robust in that these methods
  consistently outperformed the equal bin reference approach for all test
  seasons.
  %% which assigns equal probability to each possible outcome.
  % Conclusion:
  We conclude that the endemic-epidemic beta model is a performant and
  easy-to-implement tool to forecast flu activity a few weeks ahead.
  Real-time forecasting of seasonal targets, however, should consider
  outputs of multiple models simultaneously, weighing their usefulness as
  the season progresses.
}

% Keywords
\keyword{influenza; forecasting; time series; proper scoring rules; seasonality}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Influenza is a contagious respiratory illness caused by different types of influenza viruses.
The outcomes of flu infection vary widely and serious infection can cause hospitalization or death.
The Centers for Disease Control and Prevention (CDC) in the U.S. estimate that around 8\% of the U.S.
population become infected with influenza during an average season \cite{Tokars2017}.
Accurate prediction of flu activity provides health officials with valuable information to plan
disease prevention and allocate treatment resources.
Since 2013, CDC organizes the "Predict the Influenza Season Challenge"
(https://predict.cdc.gov/, a.k.a. CDC FluSight challenge)
for every flu season, to encourage academic and private industry researchers to forecast
the national and regional flu activity.
Various approaches have been used in influenza forecasting \cite{nsoesie.etal2014, chretien.etal2014}.
Reich et al. \cite{Reich2019} compare the forecast accuracy of 22 models from five different institutions.
Biggerstaff et al. \cite{biggerstaff.etal2018} present the result of a recent FluSight challenge.
Some common approaches can be grouped into following categories \cite{brooks.etal2018}:
compartmental models \cite{shaman.karspeck2012, hickmann.etal2015}, agent-based models,
direct regression models \cite{ray.etal2017, Brooks2015} and time series models \cite{hyndman.khandakar2008,dunsmuir.scott2015, held.etal2005, beta:theory}.

Here we focus on time series models for routinely available public health
surveillance data. A state-of-the-art approach to model infectious disease
counts over time is the endemic-epidemic modeling framework introduced by
Held et al. \cite[``HHH'']{held.etal2005}.
In the HHH framework, the disease incidence is divided into two components: an endemic component
which models seasonal variations of the background risk, and an epidemic component which
adds autoregressive effects to capture disease spread.
Estimation, simulation and visualization of HHH models are implemented in the
R package \texttt{surveillance} \cite{meyer.etal2017}, which has enabled
a wide range of epidemiological analyses.
However, when measuring and forecasting flu activity, the proportion of outpatient visits
with ILI is used rather than their counts, and the total number of outpatient visits is
subject to seasonal variation.
For this purpose, Lu and Meyer \cite{beta:theory} borrowed the HHH
approach and introduced an endemic-epidemic \emph{beta} model designed for
time series of infectious disease proportions.
In this model, proportions are modeled using a conditional beta distribution,
which naturally keeps the boundedness of proportions and accommodates heteroskedasticity and
potential asymmetry of proportion distributions. Likelihood inference is
straightforward via the R package \texttt{betareg} \cite{cribari-neto.zeileis2010}.
The beta model thus represents a relatively simple and fast approach to forecast proportions.

The purpose of this work is to investigate the usefulness of the
endemic-epidemic beta model as a forecasting tool.
We benchmark the beta model against several alternative methods with
readily available and well-documented implementations in R \cite{R:base}.
These methods are the seasonal autoregressive integrated moving average (SARIMA)
model \cite{hyndman.khandakar2008}, harmonic regression with ARIMA errors \cite{hyndman.khandakar2008}
and Facebook's forecasting tool Prophet \cite{Taylor2018}.
Furthermore, a successful competitor in the FluSight challenge,
kernel conditional density estimation \cite[KCDE]{ray.etal2017} is also included in our comparison.
We apply these models to predict short-term and seasonal targets, which are also forecast
targets in the FluSight challenge and relevant to public health.
The short-term targets consist of one- to four-weeks-ahead forecasts, and the seasonal targets
are prediction of the intensity and the timing of the seasonal peak.
Gneiting and Katzfuss \cite{gneiting.katzfuss2014} state that
probabilistic forecasts measure the prediction uncertainty and
thus are fundamental for decision making.
In this paper, probabilistic forecasts are evaluated by proper scoring rules,
such as the log score and the Dawid-Sebastiani score.




<<setup, include=FALSE>>=
knitr::opts_chunk$set(echo = FALSE, tidy = FALSE, results = 'markup',
                      fig.align = "center", fig.scap = NA, out.width = NULL,
                      cache = FALSE, error = FALSE, warning = FALSE, message = FALSE)
library(here)
code_path <- here("./Code/")
short_path <- here("./Results/Forecast_ph1-4/")
peak_path <- here("./Results/Peak/")

library(xtable)
two_functions = function(x){
  gsub("\\$", "\\\\$", paste("\\textbf{", x, "}", sep = ""))
}

options(xtable.sanitize.colnames.function = two_functions,
        xtable.sanitize.rownames.function = NULL,
        xtable.sanitize.text.function     = NULL)

@

%-----------------------------------------------
\section{Materials and Methods}




\subsection{Data}
\label{subsec:Data}
The U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet) collects weekly information on
outpatient visits to health care providers for influenza-like illness (ILI).
Here, ILI is defined as "fever (temperature of 100°F [37.8°C] or greater) and a cough and/or
a sore throat without a known cause other than influenza" \cite{CDCoverview}.
The national wILI index is calculated as the proportion of outpatient visits with ILI reported 
through the ILINet weighted by state population \cite{CDCoverview}. 
It is a standard measure of the national-level flu activity.

The Morbidity and Mortality Weekly Report (MMWR) week is used by the CDC to report
the wILI data \cite{MMWR}. 
The MMWR week ranges from 1 to 52, sometimes to 53.
The MMWR week 1 corresponds to the first week in the year, sometimes the last week in the last year.
In most years flu activity begins to increase at the beginning of October, 
peaks between December and February and lasts until May.
Historically, flu activity in MMWR week 40 to MMWR week 20 in the next year is 
of interest for the disease control purpose. 
The MMWR week 40 is roughly at the end of September or at the beginning of October. 
The MMWR week 20 corresponds roughly to the second or third week in May. 
In this paper, we index seasons from MMWR week 31 to MMWR week 30 in the next year. 
Accordingly season week one corresponds to MMWR week 31.

We use CDC's weekly national wILI data from season 1998/1999 to season 2017/2018 (Figure \ref{fig:wILI}) 
for our analysis. 
This data can be easily downloaded via the R package \texttt{cdcfluview} \cite{R:cdcfluview}. 
The last four seasons are taken as test data to assess the forecast performance. 
Two H1N1 pandemic seasons, 2008/2009 and 2009/2010 are excluded in our analysis, 
since we are focused on seasonal influenza forecasting .
The ILINet members continuously provide backfill reports for past weeks and consequently the previously 
issued wILI may be modified \cite{osthus.etal2019a}. 
In this paper, however, we ignore the backfill of the wILI data. 



\begin{figure}
\includegraphics[width=\textwidth]{wILI.pdf}
\centering
\caption{Weekly wILI index in the USA. In early years of data collection, 
low-season incidence was not recorded. These missing data are indicated with vertical grey bars. 
The wILI data during the excluded pandemic seasons (2008/2009 and 2009/2010) are in grey. 
The last four seasons (2014/2015 to 2017/2018) are held out as test data. 
This cutoff is indicated with a vertical dashed line.}
\label{fig:wILI}
\end{figure}


\subsection{Prediction targets and evaluation criteria}
\label{subsec:targets}

We utilize prediction targets of FluSight challenges organized by CDC \cite{biggerstaff.etal2018} 
and in \cite{ray.etal2017} for model comparison.
They can be divided into two parts: short-term targets and seasonal targets. 
The short-term targets are one-week, two-weeks, three-weeks and four-weeks ahead forecasts of wILI. 
The seasonal targets consist of peak week timing and intensity prediction. 
The peak week of an influenza season is the week which has the highest wILI during the season. 
The peak intensity is the highest value, that the wILI reaches during the influenza season.

We evaluate the model performance of short-term targets by the mean log score \cite{gneiting.etal2007},
Dawid-Sebastiani score \cite{dawid.sebastiani1999} and absolute error over the test period 
or a subset of the test period. 
We also repeat the maximum log score in \cite{ray.etal2017}.

We denote the predictive distribution by $F$, predictive density by $f$ and the actual observation by $y$. 
The log score (LS) is strictly proper and defined as

\begin{equation}
\LS(F,y) = -\log f(y).
\end{equation}

The log score in this paper is not directly comparable with the log score required by CDC 
in the FluSight challenge. 
The CDC calculates the log score of multiple bins surrounding the actual observation, 
which is nevertheless not a proper score \cite{ray.reich2018}. 

The maximum log score is a measure of worst-case performance and not proper. 
It should be only considered when a model has good overall performance 
measured by the mean log score \cite{ray.etal2017}.

The Dawid-Sebastiani score (DSS)

\begin{equation}
\DSS(F,y) = 2\log \sigma_F + (y - \mu_F)^2/\sigma_F^2
\end{equation}
is proper and depends only on the mean $\mu_F$ and variance $\sigma_F^2$ of the predictive distribution.

The absolute error (AE) is the distance between the actual observation $y$ and the point forecast $\hat{y}$,

\begin{equation}
\AEE(y,\hat{y}) = |y - \hat{y}|.
\end{equation}

For seasonal targets, we simulate 10000 trajectories starting from each week between 
season week 10 and season week 41 from each fitted model for each season in the test period. 
For each starting week, we obtain the empirical distribution of peak timing $\hat{F}_{Time}$ 
from those simulated trajectories. 
Then the log score of peak timing prediction for one starting week
is calculated as $- \log (\hat{F}_{Time}(w) - \hat{F}_{Time}(w - 1))$, where $w$ is the actual peak week.
For peak intensity prediction, we calculate the log score for binned proportion. 
The bins are $[0,0.5 \%)$, $[0.5 \%,1 \%)$,$\dots$, $[13 \%,\infty]$. 
For each starting week, we obtain the empirical distribution of binned peak week 
intensity $\hat{F}_{Int}$ from the simulated trajectories. 
The log score of peak week intensity prediction is calculated
as $- \log (\hat{F}_{Int}(I) - \hat{F}_{Int}(I - 1))$, where $I$ is the index of the
bin where the actual peak intensity lies.
The mean and maximum log score of peak timing prediction and peak intensity 
prediction over each starting week and the starting week before the actual peak 
week are used to evaluate the model performance of long-term targets.

\subsection{Endemic-epidemic \emph{beta} model}
\label{subsec:beta}
Lu and Meyer \cite{beta:theory} propose an extension of the HHH framework 
for infectious disease proportions. 
For time $t = 1,...,T$, $X_t$ is the proportion of newly infected individuals
by a certain disease at time $t$ and is assumed to follow a beta distribution
with mean $\mu_t$ and precision $\phi_t$ conditional on past observations,

\begin{equation}\label{eq:Beta1}
X_t|\mathcal{F}_{t-1} \sim \operatorname{Beta}(\mu_t,\phi_t),
\end{equation}

\begin{equation}\label{eq:Betamu1}
g(\mu_t)= \nu_t + \sum_{k = 1}^p \beta_k g(X_{t-k}),
\end{equation}
where $g(x) = \log(\frac{x}{1 - x})$ is the logit link function, 
$\mathcal{F}_{t-1} = \sigma(X_1,\dots,X_{t-1})$. 
The transformed conditional mean $g(\mu_t)$ can be decomposed into an endemic 
component $\nu_t$ and an epidemic component $\sum_{k = 1}^p \beta_k g(X_{t-k})$.
The endemic component is modeled as a linear predictor,

\begin{equation}
\nu_t = \alpha^{(v)} + \bm{\beta}^{(v)^\top}\bm{z}_t^{(v)}.
\end{equation}

The precision parameter $\phi_t$ can also be time-varying with

\begin{equation}
\log(\phi_t) = \alpha^{(\phi)} + \bm{\beta}^{(\phi)^\top}\bm{z}_t^{(\phi)}.
\end{equation}


The endemic component $\nu_t$ and the precision parameter $\phi_t$ could 
be modeled by harmonic regression plus time trends and potential holiday
effects via dummy variables \cite{held.paul2012}. 
The parameters $\bm{\beta}^{(v)}$ and $\beta_k$ can be interpreted in
terms of odds ratio \cite{beta:theory}.

This extended model is denoted by Beta($p$), where $p$ is the maximum 
order of the autoregressive terms. 
Parameter estimation can be carried out by (conditional) maximum likelihood
using the R package \texttt{betareg} \cite{cribari-neto.zeileis2010}.



\subsection{Baseline models}
\label{subsec:baselne}


Five baseline models are considered for comparison. 
The first baseline model is a SARIMA model fitted by data after logit transformation. 
We use the \emph{auto.arima} function from the R package
\texttt{forecast} \cite{hyndman.khandakar2008} to determine the order of 
the SARIMA model by a stepwise procedure.

The second baseline model is a harmonic regression model with ARIMA errors,
or ARIMA for short,
with regressors containing holiday effects via dummy variables, 
fitted by data after logit transformation. 
The number of harmonics is chosen by AICc using the training data, 
and the order of the ARIMA part is chosen by the \emph{auto.arima} function
in R package \texttt{forecast} \cite{hyndman.khandakar2008}. 

Ray et al. \cite{ray.etal2017} proposed an approach to generate predictions
of disease incidence by combining kernel conditional density estimation (KCDE) and copulas.
For each prediction horizon, predictive distributions are estimated by KCDE and 
copulas tie these predictive distribution into joint distributions. 
They evaluated different KCDE model specifications and in most cases in their 
application the KCDE model with periodic kernel components and full bandwidth 
matrix has better forecast performance than the other model specifications.
Accordingly we use that as our third baseline model, 
following the implementation provided in \cite{ray.etal2017}.


Facebook’s Core Data Science team developed the automatic forecasting procedure Prophet
and implemented it in the R package \texttt{prophet} \cite{Taylor2018}. 
Since the Prophet model assumes Gaussian errors, we fit data on the logit scale. 
Same holiday effects as in the beta model and the ARIMA model are taken into account.

The fifth baseline model is a naive approach: for each test season, 
we estimate a logitnormal distribution of wILI based on the same 
calendar week in previous seasons.


For seasonal targets, we consider an additional reference approach which 
assigns equal probability to all possible outcomes. 
For prediction of peak week timing, probability is assigned equally to 
season week 10 to 42, since historically high flu activity occurred 
between season weeks 10 and 42. For peak intensity prediction, 
equal probability is assigned to binned proportions.

The beta model, ARIMA model, SARIMA model and KCDE model are dynamic models, 
and the Prophet model, naive approach and equal bin approach are non-dynamic models.

%--------------------------------------------------------------------------------
\section{Results}
\label{sec:Results}

\subsection{Model selection}

\begin{figure}
\includegraphics[width=\textwidth]{wILIholiday.pdf}
\centering
\caption{Weighted influenza-like illness (wILI) in the USA for 
flu seasons 1998 through 2017. 
The pandemic seasons are excluded.
Season week 22 is indicated with a vertical dashed line, where a peak or 
secondary peak occurs in most seasons. Data in the training seasons and 
test seasons are in grey and black respectively. 
The peak of each season is indicated in black points.}
\label{fig:wILIholiday}
\end{figure}

In most seasons, a peak or secondary peak appears during season week 22, 
which corresponds to the MMWR week 52 (Figure \ref{fig:wILIholiday}). 
After season week 22, wILI decreases until the start of another peak. 
The peak pattern on season week 22 could be explained by the fact that
during the winter holiday patients tend not to visit the doctor for 
less severe illness, thus reducing the number of non-ILI visits and 
consequently increasing wILI \cite{brooks.etal2018}. 
During the winter holidays, the transmission of ILI is hampered 
due to a reduction of work and school contacts \cite{Hens2009}.
Then the number of ILI visits decreases towards the end of the 
winter holidays, and wILI drops during season week 23 \cite{brooks.etal2018}.
Figure \ref{fig:wILI} shows no time trend or increasing/decreasing 
variation of wILI from 1998 to 2018. 
To capture this pattern in our models, we include two dummy 
variables $x_t$ and $y_t$ for season weeks 22 and 23 respectively
and exclude time trend in the harmonic regression for the covariates
$\bm{z}_t^{(v)}$ and $\bm{z}_t^{(\phi)}$ in the beta model.
AIC is a useful criterion if prediction is the exclusive purpose \cite{shmueli2010}. 
However, AIC causes overfitting when the sample size is small or
the number of parameters is relatively large. 
In this case, AICc is a more suitable criterion \cite{HURVICH1989}\cite{Burnham2002}.
In the beta model, the number of harmonics $S_v$, $S_{\phi}$ and 
the order of autoregressive terms $p$ are chosen by AICc jointly 
using the training data. This procedure resulted in a beta model 
with $p = 4$, $S_v = 3$ and $S_{\phi} = 3$. 
Moreover, we include the beta model with $p = 1$, $S_v = 3$ 
and $S_{\phi} = 4$, which has the best AICc among the beta models with one autoregressive lag.

We use the \emph{auto.arima} function from the R package 
\texttt{forecast} \cite{hyndman.khandakar2008} to determine 
the order of the SARIMA model and the ARIMA model by a stepwise procedure. 
This procedure resulted in a SARIMA(1,0,0)(1,1,0)[52] 
and an ARIMA(5,1,0) model with $S = 4$.


We implemented the beta model and baseline models introduced in Section \ref{subsec:baselne}
to compare their forecast performance. For beta and (S)ARIMA, 
training data are used to determine the structure of the model
(e.g. order of AR and MA terms, number of harmonics). 
In the test period, parameters are reestimated given 
the observations until the specified time point in test
data while the structure of model is kept. 
For KCDE, we implement the approach provided by Ray et.al \cite{ray.etal2017}, 
which does not perform reestimation. 
Indeed, it is cumbersome to reestimate parameters for KCDE since it takes 
around 4 hours to estimate parameters.
The structure and parameters of the Prophet model are updated 
through the test period since Prophet needs to updates its change 
points when there are more observations available. 

\subsection{Short-term targets}%------------------------------------------------------

The overall short-term performance is summarized in Table \ref{tab:res_ph1_4_2}.
We consider two kinds of subsets in the short-term forecast performance comparison. 
The "all weeks" subset summarizes the mean log score and mean Dawid-Sebastiani
score over the test period. 
The "MMWR weeks 40 - 20" subset summarizes the mean log score and mean 
Dawid-Sebastiani score over MMWR weeks 40 - 20 in the test period,
which are of more interest for disease control purpose. 

For the "all weeks" subset KCDE has the best Dawid-Sebastiani score, 
the lowest maximum log score and the lowest absolute error. 
The Beta(4) model has the best mean log score and second-best forecast
performance in terms of maximum log score, absolute error and Dawid-Sebastiani score.
Forecasting with Beta(4) is considerably faster than with KCDE and has 
fewer parameters to estimate. Then follow Beta(1), ARIMA and SARIMA. 
Prophet and naive have the worst short-term forecast performance. 
When considering scores only over the MMWR weeks 40-20, 
it results in very similar rankings. 
Beta(4) has better mean log score on average than Beta(1) for both subsets.

The forecast performance of Beta(4) decreases over prediction horizons
(Table \ref{tab:res_ph1_4}). 
For one-week-ahead forecast, Beta(4) outperforms all other models in terms 
of mean log score and Dawid-Sebastiani score for both subsets. 
For higher prediction horizons and in both subsets, KCDE has the best or 
close to the best mean log score and Dawid-Sebatiani score, while Beta(4)
is always the second-best, except that for two-weeks ahead forecast
Beta(4) has the best mean log score.

Seasonal summaries of the log score difference between Beta(4) and Beta(1),
KCDE or ARIMA respectively in one- to four-weeks-ahead forecasts for 
subset "MMWR weeks 40 - 20" are shown in Figure \ref{fig:STbox}. 
For higher prediction horizons, KCDE outperforms Beta(4) in all but the last test season.




<<results="asis", echo = FALSE>>=
res_ph1_4_2 <- readRDS(paste0(short_path,"res_ph1_4_2.rds"))

print(xtable(res_ph1_4_2, align = "ccc|rrrrrr",
             caption = 'Summaries of model performance over one-week, two-weeks, 
             three-weeks and four-weeks ahead forecast. Ranks are shown in bracket. 
             The "all weeks" group summarizes the mean log score (LS) and the mean
             Dawid-Sebastiani score (DSS) over the test period. 
             The "MMWR weeks 40 - 20" group summarizes the mean log score (LS) and
             the mean Dawid-Sebastiani score (DSS) over MMWR weeks 40 - 20 in the test period.
             The total run time for estimation, reestimation
             and forecasting at all time points for all prediction horizons is given in minutes.
             For KCDE, the total run time does not include reestimation.',
             digits = c(0,0,0,2,2,2,2,0,0),
             label = "tab:res_ph1_4_2"),
      type = "latex",
      include.colnames = TRUE,
      include.rownames = FALSE,
      hline.after = c(-1, 0, 7, 14))

@

<<xtable, results="asis", echo = FALSE>>=
res_ph1_4 <- readRDS(paste0(short_path,"res_ph1_4.rds"))
source(file = paste0(code_path, "print_2heading_xtable.R"))
print_2heading_xtable(res_ph1_4,
                      separator = "__",
                      xtable.digits = 2,
                      xtable.align = "ccc|rr|rr|rr|rr",
                      xtable.caption = 'Summaries of model performance for one-week, 
                      two-weeks, three-weeks and four-weeks ahead forecast. 
                      Ranks are shown in bracket. The "all weeks" group summarizes 
                      the mean log score (LS) and the mean Dawid-Sebastiani score (DSS)
                      over the test period. The "MMWR week 40 - 20" group summarizes 
                      the mean log score (LS) and the mean Dawid-Sebastiani score (DSS)
                      over MMWR weeks 40 - 20 in the test period.',
                      xtable.label = "tab:res_ph1_4",
                      hline.after = c(-1, 0, 7, 14),
                      scalebox='0.9'
)

@



\begin{figure}
\includegraphics[width=\textwidth]{STbox.pdf}
\centering
\caption{A summary of the log score difference between Beta(4) and Beta(1), 
KCDE or ARIMA for prediction horizons one to four in the "MMWR weeks 40-20" subset in the test period. 
Positive values indicate that Beta(4) outperforms baseline models. 
The mean is marked with dots.}
\label{fig:STbox}
\end{figure}


\subsection{Seasonal targets}

For seasonal targets, we summarize the model performance for two subsets: "all weeks" and "before peak". 
The "all week" subset summarizes the mean log score and maximum log score for 
all predictions in the test period. 
The "before peak" subset summarizes the mean log score and maximum log score 
for predictions in weeks before the actual peak for the given season. 
In the FluSight challenges the forecast performance of the 
"all week" subset is evaluated, while in practice accurate prediction of 
peak timing and peak intensity before the actual peak occurs is more 
meaningful to health officials \cite{ray.reich2018}.

The model performance for seasonal targets is summarized in Table \ref{tab:res_peak}. 
In terms of mean log score, equal bin has the worst forecast performance
for both seasonal targets and in two different subsets.
For peak intensity prediction and in "all weeks" subset, 
KCDE has the best performance in terms of mean log score. 
Naive has the second-best performance. 
Then follow Beta(1), Beta(4), SARIMA, ARIMA and Prophet. 
The rank of models is the same in the "before peak" subset.
For peak timing prediction, KCDE has the best mean log score in the "all weeks" subset. 
Then follow Prophet, ARIMA, naive, Beta(4) and Beta(1). 
In the "before peak" subset, ARIMA has the best forecast performance and
KCDE follows close behind. The rank of other models is similar.

We now focus on the forecast performance in the subset "before peak"
during each test seasons.
Season 2014/2015 has a standard epidemic curve (Figure \ref{fig:wILIholiday}
and Figure \ref{fig:Peakbox}).
All models show better forecast performance than equal bin 
in both peak intensity and timing prediction. 
Prophet has the best peak intensity prediction, and SARIMA
has the best peak timing prediction.

In season 2015/2016, the peak intensity was lower and the peak occurred later 
than the most training seasons.
For peak intensity prediction, KCDE scores best and only Prophet is worse than equal bin.
The peak timing prediction is more difficult in this season. 
KCDE ranks the first and is followed by naive. Only these two models are better than equal bin.

Season 2016/2017 is a relatively normal season. 
All models have better prediction than equal bin for both targets. 
Two non-dynamic models, Prophet and naive, have better peak intensity predictions than other models.
For peak timing prediction, KCDE has the best score and is close followed by prophet and naive. 

Season 2017/2018 has a normal peak timing and relatively high peak intensity.
KCDE has the best peak intensity prediction. Then follow Beta(1), ARIMA and naive. 
Other models have worse score than equal bin. 
All models have better peak timing prediction 
than equal bin and Beta(1) ranks first.
For all test seasons and both seasonal targets, only KCDE and naive have consistently 
better forecast performance than equal bin.
Additionally, no model is consistently better than naive. 

<<results="asis", echo = FALSE>>=
res_peak <- readRDS(paste0(peak_path,"res_peak.rds"))
source(file = paste0(code_path, "print_2heading_xtable.R"))
print_2heading_xtable(res_peak,
                      separator = "__",
                      xtable.digits = 2,
                      xtable.align = "ccc|ll|ll",
                      xtable.caption = 'Summaries of model performance for predictions of 
                      peak week intensity and peak week timing. 
                      The "all weeks" group summarizes the mean log score and maximum 
                      log score for all predictions in the test period. 
                      The "before peak" group summarizes the mean log score and 
                      maximum log score for predictions in weeks before the actual 
                      peak for the given season.',
                      xtable.label = "tab:res_peak",
                      hline.after = c(-1, 0, 8, 16)
)

@


\begin{figure}
\includegraphics[width=\textwidth]{Peakbox.pdf}
\centering
\caption{Each boxplot (top) summarizes predictions made by a model in subset "before peak".
The vertical axis is the difference in log score between the given model 
and the equal bin approach. 
The mean is indicated in dots. 
Positive values indicate the case when the model outperforms the equal bin approach.
The plot (bottom) shows the trajectory of incidence over each season.}
\label{fig:Peakbox}
\end{figure}



%----------------------------------------------------
\section{Discussion}
\label{sec:Diss}
In this work, the forecast performance for both short-term and seasonal 
targets of Beta(4) is compared with some commonly used models and a competitive model, the KCDE model.

In short-term prediction for both subsets, Beta(4) has the best 
one-week ahead forecast performance in terms of mean log score and Dawid-Sebastiani score. 
KCDE has the best model performance for higher prediction horizons 
and Beta(4) is the second-best, while KCDE has higher model complexity and around
93 times higher run time than Beta(4). 
The forecast performance of Beta(4) decreases over prediction horizon. 
Non-dynamic models, Prophet and naive, do not show any competence in the short-term forecast.
Moreover, the performance of short-term forecast of the beta model is improved by increasing the 
number of autoregressive lags from one to four. 

For seasonal targets, all models have better forecast performance than equal bin
on average in terms of mean log score. 
Depending on the epidemic curve in each season, relaive performance varies 
with test seasons. 
For peak intensity prediction, two non-dynamic models, Prophet and naive, show better forecast 
performance in normal seasons than in the other seasons, e.g. in season 2014/2015 and season 2016/2017. 


KCDE is the only model which is consistently better
than equal bin across all test seasons and for both targets. 
The Beta(4) and Beta(1) models do not show any advantages for seasonal targets.

We only include well-documented models and KCDE in our model comparison. 
There are some other competitive models during the FluSight challenge, 
such as the delta density method \cite{brooks.etal2018}, 
the empirical Bayes framework \cite{Brooks2015}, the dynamic Bayesian model \cite{osthus.etal2019}, etc.
Some time series models for proportions are also applicable to flu activity forecasting, 
such as the $\beta$ARMA model \cite{rocha.cribari-neto2008} and the marginal beta regression
time series model \cite{Guolo2014}. 
Makridakis et.al. \cite{makridakis.etal2018} evaluated the performance of statistical 
and machine learning forecasting methods using a large monthly time series and observed
that machine learning methods are dominated by classical statistical methods for both
long-term and short-term forecasts. 
Thus we do not include machine learning methods in our comparison. 

Forecast performance could be improved by internet-based 
nowcast \cite{osthus.etal2019a},  backfill models \cite{brooks.etal2018} and 
multi-model ensemble \cite{Reich2019}\cite{ray.reich2018}.
As discussed in Section \ref{sec:Diss}, non-dynamic models have shown relatively
good prediction performance for seasonal targets during standard seasons, 
while some dynamic models are more competitive during other seasons. 
Thus multi-model ensemble with time-varying weights, e.g. \cite{ray.reich2018}, 
is promising to combine the advantages of both kinds of models. 
More weight could be assigned to non-dynamic models when available observations 
indicate a standard season and vice versa. 


%Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.


\section{Conclusions}

% (This section is mandatory for IJERPH... brief summary...)
In this paper, we compare the forecast performance of the beta model with 
some well-implemented baseline models. 
The beta model performs relatively better in short-term forecasting 
than most of the baseline models and has a simple model structure and short run time. 
By increasing the lags of the beta model, the short-term forecast performance of 
the beta model can be improved. 
For seasonal forecasts, the beta model does not show any advantages over baseline models. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{conceptualization, S.M.; methodology, J.L. and S.M.; formal analysis, J.L.;  writing--original draft preparation, J.L.; writing--review and editing, S.M.; visualization, J.L.; supervision, S.M.; project administration, S.M.; funding acquisition, S.M.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\funding{This research was funded by the Interdisciplinary Center for Clinical Research (IZKF) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), project J75.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{The authors thank the Centers for Disease Control and Prevention (CDC) in the U.S. for organizing the "Predict the Influenza Season Challenge" and providing influenza incidence data. Junyi Lu performed the present work in partial fulfilment of the requirements for obtaining the degree ‘Dr. rer. biol. hum.’ at the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}
\externalbibliography{yes}
\bibliography{references_Forecast}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%% Local Variables:
%% ess-nuke-trailing-whitespace-p: nil
%% End:
