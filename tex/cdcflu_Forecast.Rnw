\documentclass[ijerph,article,submit,moreauthors,pdftex]{Definitions/mdpi}
%% For supplementary material, replace "article" by "supfile"
%% For arXiv, use:
%% \documentclass[preprints,article,accept,moreauthors,pdftex]{Definitions/mdpi}
%% \pdfoutput=1

%=================================================================
\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
%\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers
%\continuouspages{yes}

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz
\usepackage{bm}
\graphicspath{{../Results/plots/}}
%=================================================================
% Full title of the paper (Capitalized)
\Title{Forecasting Flu Activity in the United States: Benchmarking an Endemic-Epidemic Beta Model}

% Author Orchid ID: enter ID or remove command
%\newcommand{\orcidauthorA}{0000-0000-000-000X}
\newcommand{\orcidauthorB}{0000-0002-1791-9449}

% Authors, for the paper (add full first names)
\Author{Junyi Lu and Sebastian Meyer *\orcidB{}}

% Authors, for metadata in PDF
\AuthorNames{Junyi Lu and Sebastian Meyer}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
Institute of Medical Informatics, Biometry, and Epidemiology,
Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg,
Erlangen, Germany}

% Contact information of the corresponding author
\corres{Correspondence: seb.meyer@fau.de}

% Abstract (max. 200 words, no \\)
\abstract{
  % Background:
  Accurate prediction of flu activity enables health officials to plan
  disease prevention and allocate treatment resources.
  % Methods:
  A promising forecasting approach is to adapt the well-established
  endemic-epidemic modelling framework to time series of infectious
  disease proportions.
  Using U.S. influenza-like illness surveillance data over 18 seasons,
  we assessed probabilistic forecasts by this new beta autoregressive
  model and by other readily available forecasting tools, including
  Prophet and (S)ARIMA, and a competitive model, kernel
  condition density estimation (KCDE).
  % Results:
  Short-term flu activity up to four weeks ahead was best predicted by the
  beta model with four autoregressive lags or by more involved KCDE.
  Non-dynamic models, Prophet and naive historical forecast, have the 
  worst scores.
  For seasonal peak prediction, only KCDE consistently outperformed the 
  equal bin appoach which assigns equal probablity to each possible outcomes.
  In normal seasons, two non-dynamic models have particularly good forecast 
  performance, while in other seasons some of the dynamic models have better scores. 
  The beta model doesn't show any competence.
  % Conclusion:
  We conclude that the endemic-epidemic beta model is a performant and
  easy-to-implement tool to forecast flu activity a few weeks ahead.
  Real-time forecasting of seasonal targets, however, should consider
  outputs of multiple models simultaneously, weighing their usefulness as
  the season progresses.
}

% Keywords
\keyword{influenza; forecasting; time series; proper scoring rules; seasonality}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Influenza is a contagious respiratory illness caused by different types of influenza viruses.
The outcomes of flu infection can vary widely and serious infection can cause hospitalization or death. The Centers for Disease Control and Prevention (CDC) in the U.S. estimate that around 8\% of the U.S. population gets sick from influenza during an average season \cite{Tokars2017}. Accurate prediction of flu activity provides health officials with valuable information to plan disease prevention and allocate treatment resources. The U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet) collects weekly information on outpatient visits to health care providers for influenza-like illness (ILI). Here, ILI is defined as "fever (temperature of 100°F [37.8°C] or greater) and a cough and/or a sore throat without a known cause other than influenza" \cite{CDCoverview}. Since 2013, CDC organizes the "Predict the Influenza Season Challenge" (https://predict.cdc.gov/) for every flu season, to encourage academic and private industry researchers to forecast flu activity \cite{biggerstaff.etal2018}. The flu activity levels are based on the national weighted influenza-like illness (wILI) index, which is the proportion of outpatient visits with ILI reported through the ILINet weighted by state population \cite{CDCoverview}.

Lu and Meyer \cite{beta:theory} introduce an endemic-epidemic beta model for infectious disease proportions, which is an extension of the HHH modeling framework \cite{held.etal2005} for infectious disease counts. In the HHH framework, the disease incidence is divided into two components: an endemic component which models time trend and seasonal variations, and an epidemic (autoregressive) component which describes the spread of disease over time. The well-documented R package \texttt{surveillance} \cite{meyer.etal2017} offers methods for visualization, likelihood inference and simulation of HHH models.
However, when measuring and forecasting the flu activity, the proportion of outpatient visits with ILI is used rather than their counts, and the total number of outpatient visits may be subject to seasonal variation. In the beta model, proportions are typically modeled using a conditional beta distribution, which naturally keeps the boundedness of proportions and accommodates heteroskedasticity and asymmetry of proportion distributions \cite{cribari-neto.zeileis2010}.


The endemic-epidemic beta model is a relatively simple and fast approach to forecast proportions, which allows for standard likelihood inference as implemented in the R package \texttt{betareg} \cite{cribari-neto.zeileis2010}. We consider some alternative methods with well-documented implementations and available R package, and compare their forecast performance with the beta model using the wILI index from 1998 to 2018 provided by CDC in the U.S. These methods are seasonal autoregressive integrated moving average (SARIMA) model \cite{hyndman.khandakar2008}, harmonic regression with ARIMA errors \cite{hyndman.khandakar2008} and Prophet model by Facebook \cite{Taylor2018}. Furthermore, a successful competitor in the flu activity forecast challenge, the kernel conditional density estimation (KCDE) method \cite{ray.etal2017} is also included in our comparison.
We apply these models to predict short-term and seasonal targets, which are also forecast targets in the flu activity forecast challenge and relevant to public health. The short-term targets consist of one- to four-weeks-ahead forecasts, and the seasonal targets are peak intensity and peak timing prediction. Gneiting and Katzfuss \cite{gneiting.katzfuss2014} state that "probabilistic forecasts serve to quantify the uncertainty in a prediction, and they are an essential ingredient of optimal decision making". In this paper, probabilistic forecasts are made for both short-term and seasonal targets and evaluated by proper scoring rules, such as the log score and the Dawid-Sebastiani score.




<<setup, include=FALSE>>=
knitr::opts_chunk$set(echo = FALSE, tidy = FALSE, results = 'markup',
                      fig.align = "center", fig.scap = NA, out.width = NULL,
                      cache = FALSE, error = FALSE, warning = FALSE, message = FALSE)
library(here)
code_path <- here("./Code/")
short_path <- here("./Results/Forecast_ph1-4/")
peak_path <- here("./Results/Peak/")

library(xtable)
two_functions = function(x){
  gsub("\\$", "\\\\$", paste("\\textbf{", x, "}", sep = ""))
}

options(xtable.sanitize.colnames.function = two_functions,
        xtable.sanitize.rownames.function = NULL,
        xtable.sanitize.text.function     = NULL)

@

%-----------------------------------------------
\section{Materials and Methods}




\subsection{Data}
\label{subsec:Data}

The Centers for Disease Control and Prevention (CDC) in the U.S. collect weekly data on the total number of patients and the number of those patients with influenza-like illness (ILI) through the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet), which consists of outpatient healthcare providers in all states of the U.S. The national weighted influenza-like illness (wILI) index is the proportion of outpatient visits with ILI weighted by state population \cite{CDCoverview}.

The Morbidity and Mortality Weekly Report (MMWR) week is used by the CDC to report disease incidence \cite{MMWR}. The MMWR week ranges from 1 to 52, sometimes to 53. The MMWR week 1 corresponds to the first week in the year, sometimes the last week in the last year. In most years flu activity begins to increase at the beginning of October, peaks between December and February and lasts until May. Historically, flu activity in MMWR week 40 to MMWR week 20 in the next year is of interest for the disease control purpose. The MMWR week 40 is roughly at the end of September or at the beginning of October. The MMWR week 20 corresponds roughly to the second or third week in May. In this paper, we index seasons from MMWR week 31 to MMWR week 30 in the next year. Then season week one corresponds to MMWR week 31.

We use CDC's weekly wILI index data from season 1998/1999 to season 2017/2018 (Figure \ref{fig:wILI}) for our analysis. The data can be easily downloaded via the R package \texttt{cdcfluview} \cite{R:cdcfluview}. The last four seasons are used as testing data to assess the forecast performance. Since we are focused on seasonal influenza, the two H1N1 pandemic seasons, 2008/2009 and 2009/2010 are excluded in our analysis. The ILINet members continuously provide backfill reports for past weeks and the previously issued wILI may be modified \cite{osthus.etal2019a}. In this paper, we ignore the backfill of the wILI data. 



\begin{figure}
\includegraphics[width=\textwidth]{wILI.pdf}
\centering
\caption{Weekly wILI index in the USA. In early years of data collection, low-season incidence was not recorded. These missing data are indicated with vertical grey bars. The wILI index data during the excluded pandemic seasons (2008/2009 and 2009/2010) are in grey. The last four seasons (2014/2015 to 2017/2018) are held out as testing data. This cutoff is indicated with a vertical dashed line.}
\label{fig:wILI}
\end{figure}


\subsection{Prediction targets and evaluation criteria}
\label{subsec:targets}

We use prediction targets of FluSight challenges organized by CDC \cite{biggerstaff.etal2018} and used by Ray et al. \cite{ray.etal2017} for model comparison. They can be divided into two parts: short-term targets and seasonal targets. The short-term targets are one-week, two-weeks, three-weeks and four-weeks ahead forecasts of wILI. The seasonal targets consist of peak week timing and intensity prediction. The peak week of an influenza season is the week which has the highest wILI during the season. The peak intensity is the highest value, that the wILI reaches during the influenza season.

We evaluate the model performance of short-term target by using the mean log score \cite{gneiting.etal2007}, Dawid-Sebastiani score \cite{dawid.sebastiani1999} and the absolute error averaged over the testing period or a subset of the testing period. We also repeat the maximum log score in \cite{ray.etal2017}.

We denote the predictive distribution by $F$, predictive density by $f$ and the actual observation by $y$. The log score (LS) is strictly proper and defined as

\begin{equation}
LS(F,y) = -\log f(y).
\end{equation}

The log score in this paper is not directly comparable with the log score required by CDC in the flu activity forecast challenge. The CDC calculates the log score of multiple bins surrounding the actual observation, which is however not a proper score \cite{ray.reich2018}. 

The maximum log score is not proper. It is a measure of worst-case performance and should be considered as a secondary measure to characterize models with good overall performance measure by the mean log score \cite{ray.etal2017}.

The Dawid-Sebastiani score (DSS)

\begin{equation}
DSS(F,y) = 2\log \sigma_F + (y - \mu_F)^2/\sigma_F^2
\end{equation}
is proper and depends only on the mean $\mu_F$ and variance $\sigma_F^2$ of the predictive distribution.

The absolute error (AE) is the distance between the actual observation $y$ and the point forecast $\hat{y}$,

\begin{equation}
AE(y,\hat{y}) = |y - \hat{y}|.
\end{equation}

For seasonal targets, we simulate 10000 trajectories starting from each week between season week 10 and season week 41 from each fitted model for each season in the testing period. For each starting week, we obtain the empirical distribution of peak timing $\hat{F}_{Time}$ from those simulated trajectories. Then the log score of peak timing prediction for one starting week is $- \log (\hat{F}_{Time}(w) - \hat{F}_{Time}(w - 1))$, where $w$ is the actual peak week.
For peak intensity prediction, we calculate the log score for binned incidence. The bins are $[0,0.5)$, $[0.5,1)$,$\dots$, $[13,\infty]$. For each starting week, we obtain the empirical distribution of binned peak week intensity $\hat{F}_{Int}$ from the simulated trajectories. The log score of peak week intensity prediction is calculated as $- \log (\hat{F}_{Int}(I) - \hat{F}_{Int}(I - 1))$, where $I$ is the index of the bin where the actual peak intensity lies. The mean and maximum of log score of peak timing prediction and peak intensity prediction over each starting week and the starting week before the actual peak week are used to evaluate the model performance of long-term targets.

\subsection{Endemic-epidemic beta model}
\label{subsec:beta}
Lu and Meyer \cite{beta:theory} propose an extension of the HHH framework for infectious disease proportions, by using the beta distribution. For time $t = 1,...,T$, $X_t$ is the proportion of newly infected individuals by a certain disease at time $t$ and is assumed to follow a beta distribution with mean $\mu_t$ and precision $\phi_t$ conditional on past observations,

\begin{equation}\label{eq:Beta1}
X_t|\mathcal{F}_{t-1} \sim \operatorname{Beta}(\mu_t,\phi_t),
\end{equation}

\begin{equation}\label{eq:Betamu1}
g(\mu_t)= \nu_t + \sum_{k = 1}^p \beta_k g(X_{t-k}),
\end{equation}
where $g(x) = \log(\frac{x}{1 - x})$ is the logit link function, $\mathcal{F}_{t-1} = \sigma(X_1,\dots,X_{t-1})$. The transformed conditional mean $\mu_t$ can be decomposed into an endemic component $\nu_t$ and an epidemic component $\sum_{k = 1}^p \beta_k g(X_{t-k})$.
The endemic component is modeled as a linear predictor,

\begin{equation}
\nu_t = \alpha^{(v)} + \bm{\beta}^{(v)^T}\bm{z}_t^{(v)}.
\end{equation}

The precision parameter $\phi_t$ can also be time-varying with

\begin{equation}
\log(\phi_t) = \alpha^{(\phi)} + \bm{\beta}^{(\phi)^T}\bm{z}_t^{(\phi)}.
\end{equation}


The endemic component $\nu_t$ and the precision parameter $\phi_t$ could be modeled by harmonic regression plus time trends and potential holiday effects via dummy variables \cite{held.paul2012}. The parameters $\bm{\beta}^{(v)}$ and $\beta_k$ can be interpreted in terms of odds ratio \cite{beta:theory}.

This extended model is denoted by Beta($p$), where $p$ is the maximum order of the autoregressive terms. Parameter estimation can be carried out by (conditional) maximum likelihood using the R package \texttt{betareg} \cite{cribari-neto.zeileis2010}.



\subsection{Baseline models}
\label{subsec:baselne}


Five baseline models are considered for comparison. The first baseline model is a SARIMA model fitted by data after logit transformation. We use the \emph{auto.arima} function from the R package \texttt{forecast} \cite{hyndman.khandakar2008} to determine the order of the SARIMA model by a stepwise procedure.

The second baseline model is a harmonic regression model with ARIMA errors, where the regressors contain holiday effects via dummy variables, fitted by data after logit transformation. We call this model ARIMA for short. The number of harmonics is chosen by AICc using the training data, and the order of the ARIMA part is chosen by the \emph{auto.arima} function in R package \texttt{forecast} \cite{hyndman.khandakar2008}. 

Ray et al. \cite{ray.etal2017} proposed an approach to generate predictions of disease incidence by using kernel conditional density estimation (KCDE) and copulas. For each prediction horizon, predictive distributions are estimated by KCDE and copulas are used to tie these predictive distribution into joint distributions. They evaluated different KCDE model specifications and in most cases in their application the KCDE model with periodic kernel components and full bandwidth matrix has better forecast performance than the other model specifications. So we use that as our third baseline model, following the implementation provided in \cite{ray.etal2017}.


Facebook’s Core Data Science team developed the automatic forecasting procedure Prophet, which is implemented in the R package \texttt{prophet} \cite{Taylor2018}. Since the Prophet model assumes Gaussian errors, we fit data on the logit scale. Same holiday effects as in the beta model and the ARIMA model are taken into account.

The fifth baseline model is a naive approach: for each testing season, we estimate a logitnormal distribution of wILI based on the same calendar week in previous seasons.


For seasonal targets, we consider an additional reference approach which assigns equal probability to all possible outcomes. For prediction of peak week timing, probability is assigned equally to season week 10 to 42 because historically high flu activity occurred between season week 10 to 42. For prediction of peak intensity, equal probability is assigned to bins $[0,0.5)$, $[0.5,1)$,$\dots$, $[13,\infty]$ percent.

The beta model, ARIMA model, SARIMA model and KCDE model are dynamic models, and the Prophet model, naive approach and the equal bin approach are non-dynamic models.

%--------------------------------------------------------------------------------
\section{Results}
\label{sec:Results}

\subsection{Model selection}

\begin{figure}
\includegraphics[width=\textwidth]{wILIholiday.pdf}
\centering
\caption{Weighted influenza-like illness (wILI) in the USA for flu seasons 1998 through 2017. The pandemic seasons are excluded. Season week 22 is indicated with a vertical dashed line, where a peak or secondary peak occurs in most seasons. Data in the training seasons and testing seasons are in grey and black respectively. The peak of each season is indicated in black points.}
\label{fig:wILIholiday}
\end{figure}

In most seasons, a peak or secondary peak occurs during season week 22, which corresponds to the MMWR week 52 (Figure \ref{fig:wILIholiday}). After season week 22, wILI decreases until the start of another peak. The peak pattern on season week 22 could be explained by the fact that during the winter holiday patients choose not to visit the doctor for less serious issues, thus reducing the number of non-ILI visits and consequently increasing wILI \cite{brooks.etal2018}. During the winter holidays, the transmission of ILI is hampered due to a reduction of work and school contacts \cite{Hens2009}. Then the number of ILI visits decreases towards the end of the winter holidays, and wILI drops during season week 23 \cite{brooks.etal2018}. Figure \ref{fig:wILI} shows no time trend or increasing/decreasing variation od wILI from 1998 to 2018. To capture this pattern in our models, we include two dummy variables $x_t$ and $y_t$ for season weeks 22 and 23 respectively and exclude time trend in the harmonic regression for the covariates $\bm{z}_t^{(v)}$ and $\bm{z}_t^{(\phi)}$ in the beta model.
AIC is a useful criterion if prediction is the sole purpose \cite{shmueli2010}. However, AIC causes overfitting when the sample size is small or the number of parameters is large. In this case, AICc is a more suitable criterion \cite{HURVICH1989}\cite{Burnham2002}.
For the beta model, the number of harmonics $S_v$, $S_{\phi}$ and the order of autoregressive terms $p$ are chosen by AICc jointly using the training data. This procedure resulted in a beta model with $p = 4$, $S_v = 3$ and $S_{\phi} = 3$. Moreover, we include the beta model with $p = 1$, $S_v = 3$ and $S_{\phi} = 4$, which has the best AICc among the beta models with $p = 1$.

We use the \emph{auto.arima} function from the R package \texttt{forecast} \cite{hyndman.khandakar2008} to determine the order of the SARIMA model and the ARIMA model by a stepwise procedure. This procedure resulted in a SARIMA(1,0,0)(1,1,0)[52] and a ARIMA(5,1,0) model with $S = 4$.


We implemented the beta model and baseline models in Section \ref{subsec:baselne} to compare their forecast performance. For beta and (S)ARIMA, training data are used to determine the structure of the model (e.g. order of AR and MA terms, number of harmonics). In the testing period, parameters are reestimated given the observations until the specified time point in testing data while the structure of model is kept. For KCDE, we implement the approach provided by Ray et.al \cite{ray.etal2017}, which does not perform reestimation. It's cumbersome to reestimate parameters for KCDE since it takes around 4 hours to estimate parameters.
The structure and parameters of the Prophet model are updated during the testing period since Prophet needs to updates its change points when there are more observations available. 

\subsection{Short-term targets}%------------------------------------------------------

The overall short-term performance is summarized in Table \ref{tab:res_ph1_4_2}. We consider two kinds of subsets in short-term forecast performance comparison. The "all weeks" subset summarizes the mean log score and the mean Dawid-Sebastiani score over the test period. The "MMWR weeks 40 - 20" subset summarizes the mean log score and the mean Dawid-Sebastiani score over MMWR weeks 40 - 20 in the testing period, which are of more interest for the disease control purpose. 

For the "all weeks" subset KCDE has the best Dawid-Sebastiani score, the lowest maximum log score and the lowest absolute error. The Beta(4) model has the best mean log score and has second-best forecast performance in terms of maximum log score, absolute error and Dawid-Sebastiani score. Forecasting with Beta(4) is considerably faster than with KCDE and has fewer parameters to estimate. Then follows Beta(1), ARIMA and SARIMA. The Prophet model and naive approach have the worst short-term forecast performance. When considering scores only over the MMWR weeks 40-20, it results in very similar rankings. The Beta(4) model has better mean log score on average than the Beta(1) model for both subsets.

The forecast performance of Beta(4) decreases over prediction horizons (Table \ref{tab:res_ph1_4}). For one-week-ahead forecasts, Beta(4) outperforms all other models in terms of mean log score and Dawid-Sebastiani score for both subsets. For higher prediction horizons and both subsets, KCDE has the best or close to the best mean log score and Dawid-Sebatiani score, while Beta(4) is always the second-best, except that for two-weeks ahead forecast Beta(4) has the best mean log score.

Seasonal summaries of the log score difference between Beta(4) and Beta(1), KCDE or ARIMA respectively in one- to four-weeks-ahead forecasts for subset "MMWR weeks 40 - 20" are shown in Figure \ref{fig:STbox}. 
For higher prediction horizons, KCDE outperforms Beta(4) in all but the last season.




<<results="asis", echo = FALSE>>=
res_ph1_4_2 <- readRDS(paste0(short_path,"res_ph1_4_2.rds"))

print(xtable(res_ph1_4_2, align = "ccc|rrrrrr",
             caption = 'Summaries of model performance over one-week, two-weeks, three-weeks and four-weeks ahead forecast. Ranks are shown in bracket. The "all weeks" group summarizes the mean log score (LS) and the mean Dawid-Sebastiani score (DSS) over the testing period. The "MMWR weeks 40 - 20" group summarizes the mean log score (LS) and the mean Dawid-Sebastiani score (DSS) over MMWR weeks 40 - 20 in the testing period. The total run time for estimation, reestimation at all test weeks and forecasting for all prediction horizons are given in minutes. For KCDE, the total run time does not include reestimation.',
             digits = c(0,0,0,2,2,2,2,0,0),
             label = "tab:res_ph1_4_2"),
      type = "latex",
      include.colnames = TRUE,
      include.rownames = FALSE,
      hline.after = c(-1, 0, 7, 14))

@

<<xtable, results="asis", echo = FALSE>>=
res_ph1_4 <- readRDS(paste0(short_path,"res_ph1_4.rds"))
source(file = paste0(code_path, "print_2heading_xtable.R"))
print_2heading_xtable(res_ph1_4,
                      separator = "__",
                      xtable.digits = 2,
                      xtable.align = "ccc|rr|rr|rr|rr",
                      xtable.caption = 'Summaries of model performance for one-week, two-weeks, three-weeks and four-weeks ahead forecast. Ranks are shown in bracket. The "all weeks" group summarizes the mean log score (LS) and the mean Dawid-Sebastiani score (DSS) over the test period. The "MMWR week 40 - 20" group summarizes the mean log score (LS) and the mean Dawid-Sebastiani score (DSS) over MMWR weeks 40 - 20 in the testing period.',
                      xtable.label = "tab:res_ph1_4",
                      hline.after = c(-1, 0, 7, 14),
                      scalebox='0.9'
)

@



\begin{figure}
\includegraphics[width=\textwidth]{STbox.pdf}
\centering
\caption{A summary of the log score difference between Beta(4) and Beta(1), KCDE or ARIMA for prediction horizons one to four over MMWR weeks 40-20 in the testing period. Positive values indicate that Beta(4) outperforms baseline models. The mean is indicated in dots.}
\label{fig:STbox}
\end{figure}


\subsection{Seasonal targets}

For seasonal targets, we summarize the model performance for two subsets: "all weeks" and "before peak". The "all week" group summarizes the mean log score and maximum log score for all predictions in the test period. The "before peak" group summarizes the mean log score and maximum log score for predictions in weeks before the actual peak for the given season. In the flu activity forecast challenges the forecast performance of the "all week" group is considered, while in practice accurate prediction of peak timing and peak intensity before the actual peak occurs is more relevant to health officials \cite{ray.reich2018}.

The model performance for seasonal targets is summarized in Table \ref{tab:res_peak}. In terms of mean log score, the equal bin approach has the worst forecast performance for both seasonal targets and two different subsets.
For peak intensity prediction, KCDE has the best performance in terms of mean log score for all predictions in the test period. The naive approach has the second-best performance. Then follow Beta(1), Beta(4), SARIMA, ARIMA and Prophet. The rank of models is the same for the "before peak" subset.
For peak timing prediction, KCDE has the best mean log score for the "all weeks" subset. Then follows Prophet, ARIMA, naive, Beta(4) and Beta(1). The ARIMA model has the best forecast performance for the "before peak" subset. Then follows KCDE, Prophet, naive approach and Beta(4).

We now focus on the forecast performance for the subset "before peak" for each testing seasons.
In season 2014/2015, the peak intensity is normal and the peak timing week is relatively early in comparison to the training seasons (Figure \ref{fig:wILIholiday} and Figure \ref{fig:Peakbox}), while in many seasons in the training seasons the wILI has peaked on that week. All models show better forecast performance than the equal bin approach in both peak intensity and peak timing prediction. The Prophet model has the best peak intensity prediction, and the SARIMA model has the best peak timing prediction.
In season 2015/2016, the peak intensity was lower and the peak occurred later than the most training seasons. For peak timing prediction, only the KCDE model shows better forecast performance than the equal bin approach in this season. Furthermore, for peak intensity prediction Prophet and the naive approach show worse forecast performance than the equal bin approach in this season. The other five dynamic models have better peak intensity prediction than equal bin. 
Season 2016/2017 is a relatively normal season. Both non-dynamic models, Prophet and naive, have better predictions for both seasonal targets than dynamic models. All models have better prediction than equal bin. 
Season 2017/2018 has a relatively high peak intensity among all training seasons. All models but KCDE has no better forecast performance than equal bin. The peak timing of this season is normal, and all models have better prediction than equal bin in peak timing prediction, while SARIMA is worse than other models.
For all testing seasons and both seasonal targets, only KCDE has consistently better forecast performance than equal bin. The naive approach has better mean log score than equal bin, but it has a worse median log score in season 2015/2016 for both long-term targets. No model is consistently better than naive. 

<<results="asis", echo = FALSE>>=
res_peak <- readRDS(paste0(peak_path,"res_peak.rds"))
source(file = paste0(code_path, "print_2heading_xtable.R"))
print_2heading_xtable(res_peak,
                      separator = "__",
                      xtable.digits = 2,
                      xtable.align = "ccc|ll|ll",
                      xtable.caption = 'Summaries of model performance for predictions of peak week intensity and peak week timing. The "all weeks" group summarizes the mean log score and maximum log score for all predictions in the test period. The "before peak" group summarizes the mean log score and maximum log score for predictions in weeks before the actual peak for the given season.',
                      xtable.label = "tab:res_peak",
                      hline.after = c(-1, 0, 8, 16)
)

@


\begin{figure}
\includegraphics[width=\textwidth]{Peakbox.pdf}
\centering
\caption{Each boxplot (left) summarizes predictions made by a model in a given season in week before the actual peak week in that season. The vertical axis is the difference in log score between the given model and the approach which assigns equal probability to each possible week or bin of the season. The mean is indicated in dots. Positive values indicate the case when the model outperforms the equal bin approach. The plot (right) shows the trajectory of incidence over each season.}
\label{fig:Peakbox}
\end{figure}



%----------------------------------------------------
\section{Discussion}
\label{sec:Diss}
In this work, the forecast performance for both short-term and seasonal targets of Beta(4) is compared with some commonly used models and a competitive model, the KCDE model.

In short-term prediction for both subsets, Beta(4) model has the best one-week ahead forecast performance in terms of mean log score and Dawid-Sebastiani score. The KCDE model has the best model performance for higher prediction horizons and Beta(4) is the second-best, while KCDE has higher model complexity and around 93 times higher run time than Beta(4) model. The forecast performance of Beta(4) decreases over prediction horizon. 
Non-dynamic models, Prophet and naive, don't show any competence in the short-term forecast.

For seasonal targets, all models have better forecast performance than equal bin on average in terms of mean log score. The non-dynamic models, Prophet and naive, show particularly good forecast performance when the prediction targets are relatively normal in comparison to training seasons, such as for peak intensity prediction in season 2014/2015, and both targets in season 2016/2017. In a season which has unusual peak timing or peak intensity, non-dynamic models will have worse forecast performance than the equal bin approach, such as for both targets in season 2015/2016 and peak intensity prediction in season 2017/2018. Some dynamic models show competence in those seasons, such as all five dynamic models in season 2015/2016 for peak intensity prediction and the KCDE model in season 2016/2016 for peak timing prediction. KCDE is the only model which is consistently better than the equal bin approach across all testing seasons and for both targets. The Beta(4) and Beta(1) models don't show any advantages for seasonal targets.

We only choose well-documented models and KCDE in our model comparison. There are some other competitive models during the flu activity forecast challenge, such as the delta density method \cite{brooks.etal2018}, the empirical Bayes framework \cite{Brooks2015}, the dynamic Bayesian model \cite{osthus.etal2019}, etc. Some time series models for proportions are also possible in flu activity forecasting, such as the $\beta$ARMA model \cite{rocha.cribari-neto2008} and the marginal beta regression time series model \cite{Guolo2014}. Makridakis et.al. \cite{makridakis.etal2018} evaluate the performance of statistical and machine learning forecasting methods using a large monthly time series and observe that machine learning methods are dominated by classical statistical methods for both long-term and short-term forecasts. Thus we don't include machine learning methods in our comparison. 

Forecast performance could be improved by augmentation with internet-based nowcast \cite{osthus.etal2019a},  backfill models \cite{brooks.etal2018} and multi-model ensemble \cite{Reich2019}\cite{ray.reich2018}. As discussed in Section \ref{sec:Diss}, non-dynamic models have shown particularly good prediction performance for seasonal targets during normal seasons, while some dynamic models are more competitive during other seasons. Thus multi-model ensemble with time-varying weights, e.g. \cite{ray.reich2018}, is promising to combine the advantages of both kinds of models. More weight could be assigned to non-dynamic models when available observations indicate a normal season and vice versa. 


%Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.


\section{Conclusions}

% (This section is mandatory for IJERPH... brief summary...)
In this paper, we compare the forecast performance of the beta model with some well-implemented baseline models. The beta model performs relatively better in short-term forecasting than most of the baseline models and has a simpler model structure and shorter run time. By increasing the lags of the beta model, the short-term forecast performance of the beta model can be improved. For seasonal forecasts, the beta model doesn't show any advantages over baseline models. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{conceptualization, S.M.; methodology, J.L. and S.M.; formal analysis, J.L.;  writing--original draft preparation, J.L.; writing--review and editing, S.M.; visualization, J.L.; supervision, S.M.; project administration, S.M.; funding acquisition, S.M.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\funding{This research was funded by the Interdisciplinary Center for Clinical Research (IZKF) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), project J75.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{The authors thank the Centers for Disease Control and Prevention (CDC) in the U.S. for organizing the "Predict the Influenza Season Challenge" and providing influenza incidence data. Junyi Lu performed the present work in partial fulfilment of the requirements for obtaining the degree ‘Dr. rer. biol. hum.’ at the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}
\externalbibliography{yes}
\bibliography{references_Forecast}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%% Local Variables:
%% ess-nuke-trailing-whitespace-p: nil
%% End:
